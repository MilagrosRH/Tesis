{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema de Markov basado en la persona con SII #\n",
    "\n",
    "Se crea un problema d emarkov ajustado al ambiente de la persona que consume ciertos alimentos y presenta ciertos sintomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from utils import argmax\n",
    "from mdp import MDP, policy_evaluation\n",
    "from bayesianFood_lite import *\n",
    "from pgmpy.sampling import BayesianModelSampling\n",
    "\n",
    "\n",
    "class PersonFoodAndSympthonsMDP(MDP):\n",
    "    def __init__(self, model, actlist, terminals, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "        #bayesian model\n",
    "        self.model= model\n",
    "        #symptoms list\n",
    "        sintomas=pd.read_csv(\"sintomas.csv\", delimiter=';')\n",
    "        # states\n",
    "        self.states = states\n",
    "        ## initial state\n",
    "        inference = BayesianModelSampling(model)\n",
    "        initial=inference.forward_sample()\n",
    "        init=[]\n",
    "        for b in sintomas['symptom']:\n",
    "            for a in list(initial):\n",
    "                if(a==b):\n",
    "                    init.append((b,initial.iloc[0][a]))\n",
    "        self.init = tuple(init)\n",
    "        #actions\n",
    "        self.actlist = actlist       \n",
    "        self.terminals = terminals\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "    def calculate_reward(self,state):\n",
    "        aux=0\n",
    "        for (s,l) in state:\n",
    "            aux=aux+l\n",
    "        \n",
    "        reward=-aux\n",
    "        return reward\n",
    "\n",
    "    def getNextState(self, a):\n",
    "        evidence = a\n",
    "        inference = BayesianModelSampling(model)\n",
    "        week=inference.likelihood_weighted_sample(evidence=evidence, size=1)\n",
    "        status=[]\n",
    "        foods=[]\n",
    "        for b in sintomas['symptom']:\n",
    "            for a in list(week):\n",
    "                if(a==b):\n",
    "                    status.append((b,week.iloc[0][a]))\n",
    "        self.reward[tuple(status)]=self.calculate_reward(status)     \n",
    "        return tuple(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\" An exploratory Q-learning agent \"\"\"\n",
    "    def __init__(self, mdp, Ne, Rplus, alpha=None):\n",
    "\n",
    "        self.gamma = mdp.gamma\n",
    "        self.terminals = mdp.terminals\n",
    "        self.all_act = mdp.actlist\n",
    "        self.Ne = Ne  # iteration limit in exploration function\n",
    "        self.Rplus = Rplus  # large value to assign before iteration limit\n",
    "        self.Q = defaultdict(float)\n",
    "        self.Nsa = defaultdict(float)\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.r = None\n",
    "\n",
    "        if alpha:\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = lambda n: 1./(1+n)  # udacity video\n",
    "\n",
    "    def f(self, u, n):\n",
    "        \"\"\" Exploration function. Returns fixed Rplus until\n",
    "        agent has visited state, action a Ne number of times.\n",
    "        Same as ADP agent in book.\"\"\"\n",
    "        if n < self.Ne:\n",
    "            return self.Rplus\n",
    "        else:\n",
    "            return u\n",
    "\n",
    "    def actions_in_state(self, state):\n",
    "        \"\"\" Return actions possible in given state.\n",
    "            Useful for max and argmax. \"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.all_act\n",
    "\n",
    "    def __call__(self, percept):\n",
    "        s1, r1 = self.update_state(percept)\n",
    "        Q, Nsa, s, a, r = self.Q, self.Nsa, self.s, self.a, self.r\n",
    "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals,\n",
    "        actions_in_state = self.actions_in_state\n",
    "        if r1 >=-2:\n",
    "            Q[s, None] = r1\n",
    "        if s is not None:\n",
    "            s1=tuple(s1)\n",
    "            a=tuple(a)\n",
    "            s=tuple(s)\n",
    "            Nsa[s, a] += 1\n",
    "            Q[s, a] += alpha*(Nsa[s, a]) * (r + gamma * max( Q[s1,tuple(a1)] for a1 in actions_in_state(s1)) - Q[s,a])\n",
    "        if r1 >=-2:\n",
    "            self.s = self.a = self.r = None\n",
    "        else:\n",
    "            self.s, self.r = s1, r1\n",
    "            self.a = argmax(actions_in_state(s1), key=lambda a1: self.f(Q[tuple(s1),tuple(a1)], Nsa[tuple(s1), tuple(a1)]))\n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        \"\"\"To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward).\"\"\"\n",
    "        return percept\n",
    "\n",
    "\n",
    "def run_single_trial(agent_program, mdp):\n",
    "    \"\"\"Execute trial for given agent_program\n",
    "    and mdp. mdp should be an instance of subclass\n",
    "    of mdp.MDP \"\"\"\n",
    "\n",
    "    def take_single_action(mdp, s, a):\n",
    "        \"\"\"\n",
    "        Get the next state with the action selected\n",
    "        \"\"\"\n",
    "        state= mdp.getNextState(a)\n",
    "\n",
    "        return state\n",
    "\n",
    "    current_state = mdp.init\n",
    "    number_states=0\n",
    "    while True:\n",
    "        current_reward = mdp.calculate_reward(current_state)\n",
    "        percept = (current_state, current_reward)\n",
    "        next_action = agent_program(percept)\n",
    "        if next_action is None:\n",
    "            print (\"El numero de estados \"+ str(number_states))\n",
    "            print (\"El reward final \"+ str(current_reward))\n",
    "            break\n",
    "        number_states+=1\n",
    "        current_state = take_single_action(mdp, current_state, next_action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickled_file = open('sintomas.p','rb')\n",
    "u=pickle.Unpickler(pickled_file)\n",
    "states = u.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('distensionAbdominal', 0),\n",
       " ('flatulencia', 0),\n",
       " ('dolorAbdominal', 0),\n",
       " ('reflujoGastroesofagico', 0),\n",
       " ('sintomasDispepsicos', 0),\n",
       " ('diarrea', 0),\n",
       " ('estrenimiento', 0),\n",
       " ('intolerancia', 0),\n",
       " ('borborigmos', 0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickled_file = open('alimentos_lite.p','rb')\n",
    "u=pickle.Unpickler(pickled_file)\n",
    "actlist = u.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symptoms list\n",
    "sintomas=pd.read_csv(\"sintomas.csv\", delimiter=';')\n",
    "terminals=[]\n",
    "terminal=[]\n",
    "for s in sintomas['symptom']:\n",
    "    terminal.append((s,0))\n",
    "terminals.append(terminal)\n",
    "i=0\n",
    "for s in sintomas['symptom']:\n",
    "    terminaln=terminal.copy()\n",
    "    terminaln[i]=(s,1)\n",
    "    i+=1\n",
    "    terminals.append(terminaln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpPerson= PersonFoodAndSympthonsMDP(model, actlist=(actlist[:10]), states=states, terminals=terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_Qlearn=QLearningAgent(mdpPerson, 5, -2, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prueba\n",
    "#a=mdpPerson.getNextState(actlist[7])\n",
    "\n",
    "#inference = BayesianModelSampling(model)\n",
    "#week=inference.likelihood_weighted_sample(evidence=evidence, size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " #mdpPerson.calculate_reward(mdpPerson.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percept = (mdpPerson.init, mdpPerson.calculate_reward(mdpPerson.init))\n",
    "#next_action = agent_Qlearn(percept)\n",
    "#print (next_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evidence = next_action\n",
    "#inference = BayesianModelSampling(model)\n",
    "#week=inference.likelihood_weighted_sample(evidence=evidence, size=1)\n",
    "#week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#st=[]\n",
    "#for b in sintomas['symptom']:\n",
    "#    for j in list(week):\n",
    "#        if(j==b):\n",
    "#            st.append((b,week.iloc[0][j]))\n",
    "#st= tuple(st)\n",
    "#st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percept = (st, mdpPerson.calculate_reward(st))\n",
    "#next_action = agent_Qlearn(percept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_single_trial(agent_Qlearn, mdpPerson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout=open('test1.txt','w')\n",
    "for i in range (10):\n",
    "    print(\"Trial: \", i)\n",
    "    print(run_single_trial(agent_Qlearn, mdpPerson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout=open('test2.txt','w')\n",
    "for i in range (50):\n",
    "    print(\"Trial: \", i)\n",
    "    print(run_single_trial(agent_Qlearn, mdpPerson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

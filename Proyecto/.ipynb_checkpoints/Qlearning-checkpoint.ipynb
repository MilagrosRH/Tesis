{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema de Markov basado en la persona con SII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea ajusta un problema de markov al problema d ela persona que ingiere alimentos y presenta ciertos sintomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from utils import argmax\n",
    "from mdp import MDP, policy_evaluation\n",
    "from bayesianFood import *\n",
    "\n",
    "\n",
    "class PersonFoodAndSympthonsMDP(MDP):\n",
    "    \n",
    "    def __init__(self, model, actlist, terminals, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "        #bayesian model\n",
    "        self.model= model\n",
    "        #symptoms list\n",
    "        sintomas=pd.read_csv(\"sintomas.csv\", delimiter=';')\n",
    "        \n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "        ## estado inicial\n",
    "        inference = BayesianModelSampling(model)\n",
    "        self.init = inference.forward_sample()\n",
    "        \n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "        \n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "    \n",
    "    def getNextState(self, a):\n",
    "        evidence = a\n",
    "        week=inference.likelihood_weighted_sample(evidence=evidence, size=1)\n",
    "        status=[]\n",
    "        foods=[]\n",
    "        for a in list(week):\n",
    "            for b in sintomas['symptom']:\n",
    "                if(a==b):\n",
    "                    status.append((a,week.iloc[0][a]))\n",
    "                \n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\" An exploratory Q-learning agent \"\"\"\n",
    "    def __init__(self, mdp, Ne, Rplus, alpha=None):\n",
    "\n",
    "        self.gamma = mdp.gamma\n",
    "        self.terminals = mdp.terminals\n",
    "        self.all_act = mdp.actlist\n",
    "        self.Ne = Ne  # iteration limit in exploration function\n",
    "        self.Rplus = Rplus  # large value to assign before iteration limit\n",
    "        self.Q = defaultdict(float)\n",
    "        self.Nsa = defaultdict(float)\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.r = None\n",
    "\n",
    "        if alpha:\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = lambda n: 1./(1+n)  # udacity video\n",
    "\n",
    "    def f(self, u, n):\n",
    "        \"\"\" Exploration function. Returns fixed Rplus until\n",
    "        agent has visited state, action a Ne number of times.\n",
    "        Same as ADP agent in book.\"\"\"\n",
    "        if n < self.Ne:\n",
    "            return self.Rplus\n",
    "        else:\n",
    "            return u\n",
    "\n",
    "    def actions_in_state(self, state):\n",
    "        \"\"\" Return actions possible in given state.\n",
    "            Useful for max and argmax. \"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.all_act\n",
    "\n",
    "    def __call__(self, percept):\n",
    "        s1, r1 = self.update_state(percept)\n",
    "        Q, Nsa, s, a, r = self.Q, self.Nsa, self.s, self.a, self.r\n",
    "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals,\n",
    "        actions_in_state = self.actions_in_state\n",
    "\n",
    "        if s in terminals:\n",
    "            Q[s, None] = r1\n",
    "        if s is not None:\n",
    "            Nsa[s, a] += 1\n",
    "            Q[s, a] += alpha(Nsa[s, a]) * (r + gamma * max(Q[s1, a1]\n",
    "                                           for a1 in actions_in_state(s1)) - Q[s, a])\n",
    "        if s in terminals:\n",
    "            self.s = self.a = self.r = None\n",
    "        else:\n",
    "            self.s, self.r = s1, r1\n",
    "            self.a = argmax(actions_in_state(s1), key=lambda a1: self.f(Q[s1, a1], Nsa[s1, a1]))\n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        \"\"\"To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward).\"\"\"\n",
    "        return percept\n",
    "\n",
    "\n",
    "def run_single_trial(agent_program, mdp):\n",
    "    \"\"\"Execute trial for given agent_program\n",
    "    and mdp. mdp should be an instance of subclass\n",
    "    of mdp.MDP \"\"\"\n",
    "\n",
    "    def take_single_action(mdp, s, a):\n",
    "        \"\"\"\n",
    "        Get the next state with the action selected\n",
    "        \"\"\"\n",
    "        state= mdp.getNextState(a)\n",
    "        return state\n",
    "\n",
    "    current_state = mdp.init\n",
    "    while True:\n",
    "        current_reward = mdp.R(current_state)\n",
    "        percept = (current_state, current_reward)\n",
    "        next_action = agent_program(percept)\n",
    "        if next_action is None:\n",
    "            break\n",
    "        current_state = take_single_action(mdp, current_state, next_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
